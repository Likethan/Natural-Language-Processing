{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAlsIKR-ilsF"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "ddJ1FIOsish8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Text preprocessing includes cleaning, stemming, and lemmatization of words to improve Natural Language Processing tasks.\"\n"
      ],
      "metadata": {
        "id": "cEujEA6Aisek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the text\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation, numbers, and special characters\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "# Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(\"Cleaned Text:\", text)\n"
      ],
      "metadata": {
        "id": "HUSDnI44iyyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ],
      "metadata": {
        "id": "UV3yxaOYi1JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed)\n"
      ],
      "metadata": {
        "id": "KySlIteKi3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Lemmatized Words:\", lemmatized)\n"
      ],
      "metadata": {
        "id": "PbUq02Bvi6iM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}